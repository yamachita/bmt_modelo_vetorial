{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yama/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "import logging as log\n",
    "import unicodedata\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from stemmer import PorterStemmer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "logger = log.getLogger()\n",
    "logger.setLevel(log.INFO)\n",
    "\n",
    "data_dir = Path('../data')\n",
    "results_dir = Path('../RESULT')\n",
    "configs_dir = Path('../configs')\n",
    "\n",
    "def config_parser(config_file_path: Path) -> dict:\n",
    "    '''Retorna dict no formato {instrução:[arquivos]}'''\n",
    "    log.info(f'Lendo arquivo de configuração {config_file_path}')\n",
    "\n",
    "    with open(config_file_path, 'r') as config_file:\n",
    "        lines = config_file.read().split('\\n')\n",
    "\n",
    "    instructions = {}\n",
    "\n",
    "    stemmer_options = ['STEMMER', 'NOSTEMMER']\n",
    "    if lines[0] in stemmer_options:\n",
    "        instructions['STEMMER'] = True if lines[0] == stemmer_options[0] else False\n",
    "        lines = lines[1:]\n",
    "\n",
    "    for k,v in [i.split('=') for i in lines]:\n",
    "        k = k.strip()\n",
    "        v = v.strip()\n",
    "        if k in instructions:\n",
    "            instructions[k].append(v)\n",
    "        else:\n",
    "            instructions[k] = [v]\n",
    "\n",
    "    log.info(f'Configurações encontradas em {config_file_path.name}:\\n{instructions}\\n')\n",
    "    \n",
    "    return instructions\n",
    "\n",
    "\n",
    "def to_csv(csv_file_path: str|Path, data: dict, headers: bool = True) -> None:\n",
    "    log.info(f'Salvando dados no arquivo {csv_file_path}')\n",
    "\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=';')\n",
    "        if headers:\n",
    "            writer.writerow(list(data.keys()))\n",
    "\n",
    "        lines = list(zip(*data.values()))\n",
    "        writer.writerows(lines)\n",
    "\n",
    "        if headers:\n",
    "            log.info(f'Cabeçalho do arquivo salvo:\\n{list(data.keys())}')\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, data: list[str]):\n",
    "        self.data = data\n",
    "\n",
    "    def load(self, data: list[str]):\n",
    "        self.data = data\n",
    "        return self\n",
    "\n",
    "    def to_upper(self):\n",
    "        log.info(f'Convertendo letras para maiúsculas...')\n",
    "        self.data = list(map(lambda x: x.upper(), self.data))\n",
    "        return self\n",
    "    \n",
    "    def remove_accents(self):\n",
    "        log.info(f'Removendo acentos...')\n",
    "        self.data = list(map(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8'), self.data))\n",
    "        return self\n",
    "    \n",
    "    def remove_stopwords(self, stopwords: list[str] = nltk.corpus.stopwords.words('english')):\n",
    "        log.info(f'Removendo stopwords...')\n",
    "        self.data = list(map(\n",
    "            lambda x: ' '.join([ word for word in x.split() if word.lower() not in stopwords]),\n",
    "            self.data))\n",
    "        return self\n",
    "    \n",
    "    def remove_punctuation(self):\n",
    "        log.info(f'Removendo pontuação...')\n",
    "        self.data = list(map(lambda x: x.translate(str.maketrans('', '', string.punctuation)), self.data))\n",
    "        return self\n",
    "    \n",
    "    def remove_escape_sequences(self):\n",
    "        log.info(f'Removendo sequências de escapes...')\n",
    "        self.data = list(map(lambda x: re.sub(r'\\s+',' ',x).strip(), self.data))\n",
    "        return self\n",
    "    \n",
    "    def stemming(self, stemmer, use=True):\n",
    "        if use:\n",
    "            log.info(f'Aplicando stemming...')\n",
    "            self.data = list(map(lambda x: ' '.join([stemmer.stem(s, 0, len(s)-1) for s in x.split()]), self.data))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processador de Consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executando processador de consultas...\n",
      "\n",
      "INFO:root:Lendo arquivo de configuração ../configs/PC.CFG\n",
      "INFO:root:Configurações encontradas em PC.CFG:\n",
      "{'STEMMER': True, 'LEIA': ['cfquery.xml'], 'CONSULTAS': ['consultas.csv'], 'ESPERADOS': ['esperados.csv']}\n",
      "\n",
      "INFO:root:Lendo arquivo de consultas em ../data/cfquery.xml\n",
      "INFO:root:Número de consultas encontradas: 99\n",
      "\n",
      "INFO:root:Processando texto das consultas...\n",
      "INFO:root:Removendo sequências de escapes...\n",
      "INFO:root:Removendo acentos...\n",
      "INFO:root:Removendo pontuação...\n",
      "INFO:root:Removendo stopwords...\n",
      "INFO:root:Aplicando stemming...\n",
      "INFO:root:Convertendo letras para maiúsculas...\n",
      "INFO:root:Processamento de texto concluído.\n",
      "\n",
      "INFO:root:Consultas processadas em 12.08ms.\n",
      "\n",
      "INFO:root:Salvando dados no arquivo ../RESULT/consultas.csv\n",
      "INFO:root:Cabeçalho do arquivo salvo:\n",
      "['QueryNumber', 'QueryText']\n",
      "INFO:root:Criando base de dados dos resultados esperados...\n",
      "INFO:root:Salvando dados no arquivo ../RESULT/esperados.csv\n",
      "INFO:root:Cabeçalho do arquivo salvo:\n",
      "['QueryNumber', 'DocNumber', 'DocVotes']\n"
     ]
    }
   ],
   "source": [
    "log.info('Executando processador de consultas...\\n')\n",
    "\n",
    "pc_config_file = configs_dir/'PC.CFG'\n",
    "pc_config = config_parser(pc_config_file)\n",
    "\n",
    "queries_source_file = data_dir/pc_config['LEIA'][0]\n",
    "queries_dest_file = results_dir/pc_config['CONSULTAS'][0]\n",
    "expected_dest_file = results_dir/pc_config['ESPERADOS'][0]\n",
    "\n",
    "\n",
    "log.info(f'Lendo arquivo de consultas em {queries_source_file}')\n",
    "queries = ET.parse(queries_source_file)\n",
    "\n",
    "queries_dict = {'QueryNumber':[], 'QueryText':[]}\n",
    "\n",
    "start = time.time()\n",
    "for query in queries.findall('QUERY'):\n",
    "    query_number = query.find('QueryNumber').text\n",
    "    query_text = query.find('QueryText').text\n",
    "    queries_dict['QueryNumber'].append(query_number)\n",
    "    queries_dict['QueryText'].append(query_text)\n",
    "\n",
    "log.info(f'Número de consultas encontradas: {len(queries_dict['QueryNumber'])}\\n')\n",
    "\n",
    "\n",
    "log.info('Processando texto das consultas...')\n",
    "stemmer = pc_config['STEMMER']\n",
    "queries_text = TextPreprocessor(queries_dict['QueryText'])\n",
    "\n",
    "queries_dict['QueryText'] = queries_text \\\n",
    "                            .remove_escape_sequences() \\\n",
    "                            .remove_accents() \\\n",
    "                            .remove_punctuation() \\\n",
    "                            .remove_stopwords() \\\n",
    "                            .stemming(PorterStemmer(), use=stemmer) \\\n",
    "                            .to_upper() \\\n",
    "                            .data\n",
    "\n",
    "end = time.time()\n",
    "log.info('Processamento de texto concluído.\\n')\n",
    "log.info(f'Consultas processadas em {(end-start) * 1000:.2f}ms.\\n')\n",
    "\n",
    "to_csv(queries_dest_file, queries_dict)\n",
    "\n",
    "# ------------------\n",
    "\n",
    "def score_to_votes(score: str) -> int:\n",
    "    num_votes = 0\n",
    "    for digit in score:\n",
    "        num_votes += int(digit)\n",
    "    return num_votes\n",
    "\n",
    "log.info(f'Criando base de dados dos resultados esperados...')\n",
    "\n",
    "expected_dict = {'QueryNumber':[], 'DocNumber':[], 'DocVotes':[]}\n",
    "\n",
    "for query in queries.findall('QUERY'):\n",
    "    query_number = query.find('QueryNumber').text\n",
    "    for item in query.find('Records'):\n",
    "        expected_dict['QueryNumber'].append(query_number)\n",
    "        expected_dict['DocNumber'].append(item.text)\n",
    "        expected_dict['DocVotes'].append(score_to_votes(item.get('score')))\n",
    "\n",
    "to_csv(expected_dest_file, expected_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerador de Lista Invertida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Executando gerador de lista invertida...\n",
      "\n",
      "INFO:root:Lendo arquivo de configuração ../configs/GLI.CFG\n",
      "INFO:root:Configurações encontradas em GLI.CFG:\n",
      "{'STEMMER': True, 'LEIA': ['cf74.xml', 'cf75.xml', 'cf76.xml', 'cf77.xml', 'cf78.xml', 'cf79.xml'], 'ESCREVA': ['li.csv']}\n",
      "\n",
      "INFO:root:Lendo diretório de documentos em /../data\n",
      "INFO:root:Lendo arquivo cf74.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 164\n",
      "INFO:root:Lendo arquivo cf75.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 185\n",
      "INFO:root:Lendo arquivo cf76.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 224\n",
      "INFO:root:Lendo arquivo cf77.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 195\n",
      "INFO:root:Lendo arquivo cf78.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 195\n",
      "INFO:root:Lendo arquivo cf79.xml...\n",
      "INFO:root:Documentos encontrados com \"Abstract\" ou \"Extract\": 252\n",
      "INFO:root:Total de documentos encontrados com \"Abstract\" ou \"Extract\": 1215\n",
      "\n",
      "INFO:root:Processando texto dos documentos...\n",
      "INFO:root:Removendo sequências de escapes...\n",
      "INFO:root:Removendo acentos...\n",
      "INFO:root:Removendo pontuação...\n",
      "INFO:root:Removendo stopwords...\n",
      "INFO:root:Aplicando stemming...\n",
      "INFO:root:Convertendo letras para maiúsculas...\n",
      "INFO:root:Processamento de texto concluído.\n",
      "\n",
      "INFO:root:Iniciando construção da lista invertida...\n",
      "INFO:root:Número de palavras únicas: 8149\n",
      "INFO:root:Lista invertida construída em 18.49ms.\n",
      "\n",
      "INFO:root:Salvando dados no arquivo ../RESULT/li.csv\n"
     ]
    }
   ],
   "source": [
    "log.info('Executando gerador de lista invertida...\\n')\n",
    "\n",
    "gli_config_file = configs_dir/'GLI.CFG'\n",
    "gli_config = config_parser(gli_config_file)\n",
    "gli_source_files = gli_config['LEIA']\n",
    "gli_dest_file = results_dir/gli_config['ESCREVA'][0]\n",
    "\n",
    "\n",
    "log.info(f'Lendo diretório de documentos em /{data_dir}')\n",
    "\n",
    "docs_dict = {'DocNumber':[], 'Abstract': []}\n",
    "\n",
    "for file in gli_source_files:\n",
    "\n",
    "    log.info(f'Lendo arquivo {file}...')\n",
    "    docs = ET.parse(data_dir/file)\n",
    "\n",
    "    doc_count = 0\n",
    "    for doc in docs.findall('RECORD'):\n",
    "        doc_number = doc.find('RECORDNUM').text\n",
    "        abstract = doc.find('ABSTRACT')\n",
    "        if abstract is None:\n",
    "            abstract = doc.find('EXTRACT')\n",
    "        if abstract is not None:\n",
    "            doc_count += 1\n",
    "            docs_dict['DocNumber'].append(int(doc_number.strip()))\n",
    "            docs_dict['Abstract'].append(abstract.text)\n",
    "    log.info(f'Documentos encontrados com \"Abstract\" ou \"Extract\": {doc_count}')\n",
    "\n",
    "log.info(f'Total de documentos encontrados com \"Abstract\" ou \"Extract\": {len(docs_dict['DocNumber'])}\\n')\n",
    "\n",
    "log.info('Processando texto dos documentos...')\n",
    "stemmer = gli_config['STEMMER']\n",
    "abstracts_text = TextPreprocessor(docs_dict['Abstract'])\n",
    "docs_dict['Abstract'] = abstracts_text \\\n",
    "                        .remove_escape_sequences() \\\n",
    "                        .remove_accents() \\\n",
    "                        .remove_punctuation() \\\n",
    "                        .remove_stopwords() \\\n",
    "                        .stemming(PorterStemmer(), use=stemmer) \\\n",
    "                        .to_upper() \\\n",
    "                        .data\n",
    "log.info('Processamento de texto concluído.\\n')\n",
    "\n",
    "\n",
    "log.info(f'Iniciando construção da lista invertida...')\n",
    "\n",
    "gli_dict = {}\n",
    "start = time.time()\n",
    "for doc_number, abstract in zip(*docs_dict.values()):\n",
    "    words = abstract.split()\n",
    "    for word in words:\n",
    "        if word in gli_dict:\n",
    "            gli_dict[word].append(doc_number)\n",
    "        else:\n",
    "            gli_dict[word] = [doc_number]\n",
    "end = time.time()\n",
    "log.info(f'Número de palavras únicas: {len(gli_dict)}')\n",
    "log.info(f'Lista invertida construída em {(end-start) * 1000:.2f}ms.\\n')\n",
    "\n",
    "to_csv(gli_dest_file, {'Word':list(gli_dict.keys()), 'WordDocs':list(gli_dict.values())}, headers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Lendo arquivo de configuração ../configs/INDEX.CFG\n",
      "INFO:root:Configurações encontradas em INDEX.CFG:\n",
      "{'LEIA': ['li.csv'], 'ESCREVA': ['modelo_vetorial.json']}\n",
      "\n",
      "INFO:root:Executando indexador...\n",
      "\n",
      "INFO:root:Criando modelo vetorial...\n",
      "INFO:root:Carregando lista invertida de /../RESULT/li.csv\n",
      "INFO:root:Normalizando lista (somente palavras com tamanho >=2 e sem dígitos)...\n",
      "INFO:root:Número de palavras únicas: 7140\n",
      "INFO:root:Criando modelo de representação tf-idf com o seguinte formato para cada documento:\n",
      "{índice_palavra1: tfidf_palavra1, índice_palavra2: tfidf_palavra2, ...}\n",
      "INFO:root:Número de documentos: 1215\n",
      "INFO:root:Modelo vetorial criado em 70.72ms.\n",
      "\n",
      "INFO:root:Salvando modelo em ../RESULT/modelo_vetorial.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Indexador:\n",
    "\n",
    "    def __init__(self, inverted_index_path: Path|str):\n",
    "\n",
    "        self.tf_normalization_factor_func = lambda x: max(x.values())\n",
    "\n",
    "        log.info('Criando modelo vetorial...')\n",
    "        start = time.time()\n",
    "        self.vector_model_tfidfs = self._create_vector_model_tfidf(inverted_index_path)\n",
    "        end = time.time()\n",
    "        log.info(f'Modelo vetorial criado em {(end-start) * 1000:.2f}ms.\\n')\n",
    "            \n",
    "\n",
    "    def save_vector_model_to_json(self, json_file_path: Path|str) -> None:\n",
    "        log.info(f'Salvando modelo em {json_file_path}\\n')\n",
    "        json_model = {'vector_model_tfidf': self.vector_model_tfidfs,\n",
    "                      'word_index': self.word_index,\n",
    "                      'word_idfs': self.word_idfs}\n",
    "        \n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump(json_model, json_file)\n",
    "\n",
    "    def _create_vector_model_tfidf(self, inverted_index_path: Path|str) -> dict:\n",
    "\n",
    "        inverted_index = self._load_inverted_index_from_csv(inverted_index_path)\n",
    "        inverted_index = self._preprocess_inverted_index(inverted_index)\n",
    "        self.word_index = self._create_word_index(inverted_index)\n",
    "        return self._create_tfidfs(inverted_index)\n",
    "\n",
    "\n",
    "    def _load_inverted_index_from_csv(self, inverted_index_path: Path|str) -> dict:\n",
    "        '''Lê o arquivo da lista invertida e armazena em um dict'''\n",
    "        log.info(f'Carregando lista invertida de /{inverted_index_path}')\n",
    "        inverted_index_dict = {}\n",
    "\n",
    "        with open(inverted_index_path) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "            for word, word_docs in csv_reader:\n",
    "                inverted_index_dict[word] = list(map(lambda x: int(x), word_docs[1:-1].split(',')))\n",
    "        return inverted_index_dict\n",
    "    \n",
    "    def _preprocess_inverted_index(self, inverted_index: dict) -> dict:\n",
    "        '''Somente palavras com tamanho >= 2 e sem números'''\n",
    "\n",
    "        log.info('Normalizando lista (somente palavras com tamanho >=2 e sem dígitos)...')\n",
    "        words = list(inverted_index.keys())\n",
    "        for word in words:\n",
    "            if len(word) < 2 or any(ch.isdigit() for ch in word):\n",
    "                del inverted_index[word]\n",
    "\n",
    "        log.info(f'Número de palavras únicas: {len(inverted_index)}')\n",
    "        return inverted_index\n",
    "    \n",
    "    def _create_word_index(self, inverted_index: dict) -> dict:\n",
    "        '''Dicionário para mapear cada palavra a um índice para o vetor de representação dos documentos'''\n",
    "        return dict(zip(list(inverted_index.keys()), list(range(len(inverted_index)))))\n",
    "    \n",
    "    \n",
    "    def _create_doc_vectors_freq(self, inverted_index: dict) -> dict:\n",
    "        doc_index_dict = {}\n",
    "        for word, word_docs in inverted_index.items():\n",
    "            for doc in word_docs:\n",
    "                word_index = self.word_index[word]\n",
    "                if doc not in doc_index_dict:\n",
    "                    doc_index_dict[doc] = {}\n",
    "                if word_index not in doc_index_dict[doc]:\n",
    "                    doc_index_dict[doc][word_index] = 1\n",
    "                else:\n",
    "                    doc_index_dict[doc][word_index] += 1\n",
    "        return doc_index_dict\n",
    "\n",
    "    def _create_idfs(self, inverted_index: dict) -> dict:\n",
    "        '''idf de cada palavra do vocabulário'''\n",
    "\n",
    "        flat_docs = []\n",
    "        for row in list(inverted_index.values()):\n",
    "            flat_docs += row\n",
    "        number_of_docs = len(set(flat_docs))\n",
    "\n",
    "        log.info(f'Número de documentos: {number_of_docs}')\n",
    "\n",
    "        idfs = {}\n",
    "        for word, word_docs in inverted_index.items():\n",
    "            df = len(set(word_docs))\n",
    "            idf = math.log10(number_of_docs/df)\n",
    "            idfs[self.word_index[word]] = idf\n",
    "        return idfs\n",
    "    \n",
    "    def set_tf_normalization_factor_func(self, tf_normalization_factor_func: Callable[[dict], int]):\n",
    "        self.tf_normaliztion_factor_func = tf_normalization_factor_func\n",
    "    \n",
    "    def _create_tfidfs(self, inverted_index: dict):\n",
    "        log.info('Criando modelo de representação tf-idf com o seguinte formato para cada documento:\\n'\\\n",
    "                 '{índice_palavra1: tfidf_palavra1, índice_palavra2: tfidf_palavra2, ...}')\n",
    "        self.word_idfs = self._create_idfs(inverted_index)\n",
    "        doc_vectors_freq = self._create_doc_vectors_freq(inverted_index)\n",
    "\n",
    "        doc_index_tfidf = {}\n",
    "\n",
    "        for doc, word_freqs in doc_vectors_freq.items():\n",
    "            doc_index_tfidf[str(doc)] = {}\n",
    "            tf_normalization_factor = self.tf_normalization_factor_func(word_freqs)\n",
    "            for word, tf in word_freqs.items():\n",
    "                ntf = tf/tf_normalization_factor\n",
    "                doc_index_tfidf[str(doc)][str(word)] = ntf * self.word_idfs[word]\n",
    "        return doc_index_tfidf\n",
    "    \n",
    "\n",
    "index_config_file = configs_dir/'INDEX.CFG'\n",
    "index_config = config_parser(index_config_file)\n",
    "inverted_index_path = results_dir/index_config['LEIA'][0]\n",
    "vector_model_json_path = results_dir/index_config['ESCREVA'][0]\n",
    "\n",
    "log.info('Executando indexador...\\n')\n",
    "idx = Indexador(inverted_index_path)\n",
    "idx.save_vector_model_to_json(vector_model_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Lendo arquivo de configuração ../configs/BUSCA.CFG\n",
      "INFO:root:Configurações encontradas em BUSCA.CFG:\n",
      "{'MODELO': ['modelo_vetorial.json'], 'CONSULTAS': ['consultas.csv'], 'RESULTADOS': ['RESULTADOS.csv']}\n",
      "\n",
      "INFO:root:Executando buscador...\n",
      "\n",
      "INFO:root:Modelo carregado de ../RESULT/modelo_vetorial.json\n",
      "\n",
      "INFO:root:Carregando consultas de ../RESULT/consultas.csv...\n",
      "INFO:root:Consultas encontradas: 99\n",
      "\n",
      "INFO:root:Buscas realizadas em 603.06ms.\n",
      "INFO:root:Tempo médio por busca 6.09ms.\n",
      "\n",
      "INFO:root:Salvando dados no arquivo ../RESULT/RESULTADOS-STEMMER.csv\n"
     ]
    }
   ],
   "source": [
    "class BuscadorModeloVetorial:\n",
    "\n",
    "    def __init__(self, vector_model_json_path: Path|str|None = None):\n",
    "        \n",
    "        with open(vector_model_json_path) as json_file:\n",
    "            model_dict = json.load(json_file)\n",
    "\n",
    "        self.vector_model_tfidfs = model_dict['vector_model_tfidf']\n",
    "        self.word_index = model_dict['word_index']\n",
    "        self.word_idfs = model_dict['word_idfs']\n",
    "        log.info(f'Modelo carregado de {vector_model_json_path}\\n')\n",
    "\n",
    "\n",
    "    def _norm2(self, vec: dict) -> float:\n",
    "        return math.sqrt(sum([x**2 for x in vec.values()]))\n",
    "\n",
    "    def _cosine_similarity(self, vec1: dict, vec2: dict) -> float:\n",
    "        vec1_norm = self._norm2(vec1)\n",
    "        vec2_norm = self._norm2(vec2)\n",
    "\n",
    "        dot_product = 0\n",
    "        for w_index, w_tfidf in vec1.items():\n",
    "            dot_product += w_tfidf * vec2.get(str(w_index), 0)\n",
    "\n",
    "        return dot_product/(vec1_norm * vec2_norm)\n",
    "\n",
    "    def find_docs(self, query: dict) -> dict:\n",
    "        distances = {}\n",
    "        for doc, tfidf_vector in self.vector_model_tfidfs.items():\n",
    "            distances.update({doc: 1-self._cosine_similarity(query, tfidf_vector)})\n",
    "        return dict(sorted(distances.items(), key=lambda x: x[1]))   \n",
    "    \n",
    "    def query_setup(self, query: list) -> dict:\n",
    "        query_dict = {}\n",
    "        for word in query:\n",
    "            w_index = self.word_index.get(word)\n",
    "            if w_index is not None:\n",
    "                query_dict[w_index] = 1\n",
    "        return query_dict\n",
    "    \n",
    "\n",
    "busca_config_file = configs_dir/'BUSCA.CFG'\n",
    "busca_config = config_parser(busca_config_file)\n",
    "vector_model_json_path = results_dir/busca_config['MODELO'][0]\n",
    "queries_file = results_dir/busca_config['CONSULTAS'][0]\n",
    "\n",
    "stm = 'STEMMER' if stemmer else 'NOSTEMMER'\n",
    "results_file = results_dir/f'{busca_config['RESULTADOS'][0].split('.')[0]}-{stm}.csv'\n",
    "\n",
    "\n",
    "\n",
    "log.info('Executando buscador...\\n')\n",
    "vm = BuscadorModeloVetorial(vector_model_json_path)\n",
    "\n",
    "log.info(f'Carregando consultas de {queries_file}...')\n",
    "queries_dict = {}\n",
    "header = True\n",
    "with open(queries_file) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "    for query_number, query_text in csv_reader:\n",
    "        if header:\n",
    "            header = False\n",
    "            continue\n",
    "        queries_dict[query_number] = vm.query_setup(query_text.split())\n",
    "\n",
    "log.info(f'Consultas encontradas: {len(queries_dict)}\\n')\n",
    "\n",
    "results_dict = {'Consulta': [], 'Ranking': []}\n",
    "\n",
    "start = time.time()\n",
    "for query_number, query in queries_dict.items():\n",
    "    docs_ranking = vm.find_docs(query)\n",
    "    result_rank = []\n",
    "    for i, (doc_number, distance ) in enumerate(docs_ranking.items(), 1):\n",
    "        result_rank.append((i, doc_number, distance))\n",
    "    results_dict['Consulta'].append(query_number)\n",
    "    results_dict['Ranking'].append(result_rank)\n",
    "end = time.time()\n",
    "log.info(f'Buscas realizadas em {(end-start) * 1000:.2f}ms.')\n",
    "log.info(f'Tempo médio por busca {((end-start) * 1000)/len(queries_dict):.2f}ms.\\n')\n",
    "\n",
    "to_csv(results_file, results_dict, headers=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
